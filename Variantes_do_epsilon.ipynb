{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christiandoramo/Variantes-para-o-epsilon/blob/main/Variantes_do_epsilon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkFF3MkxEyP"
      },
      "source": [
        "Tema 2: Variantes para o epsilon <br>\n",
        "Equipe: <br>\n",
        "Christian Oliveira <br>\n",
        "Raphael Barbosa <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGgyS0h4xoSX"
      },
      "source": [
        "Info: [tema 2](https://docs.google.com/document/d/1bpqsUML8XkUVBMjiKMLNwbG3npgaF_35xQYrh-Dp4Ts/edit?tab=t.0#heading=h.rs93iuj1gddv )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqdkF_zjzzdA"
      },
      "source": [
        "# Implementação das Variantes do ε"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtrRXXsV0JvL"
      },
      "source": [
        "##  Decaimento do ε (Linear e Exponencial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuh5ELm80k9i"
      },
      "source": [
        "### Decaimento linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aWsTV7Ixw6h"
      },
      "source": [
        "#### Função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVIFsX_7xE0E"
      },
      "outputs": [],
      "source": [
        "# testando um tipo de decaimento linear do Epsilon\n",
        "def linear_decay(initial_E=1.0, min_E=0.005, X=1000):\n",
        "    # X é o numero do passos (limite) para achar o min_E\n",
        "    k = (initial_E - min_E)/X\n",
        "    # k é a taxa de decaimento linear: Epsilon inicial - Epsilon minimo/x\n",
        "    def new_E(t): # t é o passo atual no treinamento\n",
        "        return max(min_E, initial_E - k * t)\n",
        "        # primeira vez -> (0.005, 1 - ((1-0.005)/1000) * 1) (0.005 ou 0,999005)\n",
        "        # segunda vez -> (0.005, 1 - ((1-0.005)/1000) * 2)...\n",
        "    return new_E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLQHD9gfxz37"
      },
      "source": [
        "#### Aplicado ao Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvSo9g68xs5s"
      },
      "outputs": [],
      "source": [
        "# (Q-learning é off-policy) - usando a mesma func E-greedy e apenas alterando o Q-learning do tópico recapitulações\n",
        "# ajuste dos parametros -> retira epsilon e coloca e_inicial e min, e X\n",
        "\n",
        "# Algoritmo Q-learning\n",
        "# Atenção: os espaços de estados e de ações precisam ser discretos, dados por valores inteiros\n",
        "def run_qlearning_linear_decay(env, episodes, lr=0.1, gamma=0.95, initial_E=1, min_E=0.005, X=1000):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # inicializa a tabela Q toda com zeros\n",
        "    # usar o estado como índice das linhas e a ação como índice das colunas\n",
        "    Q = np.zeros(shape = (env.observation_space.n, num_actions))\n",
        "\n",
        "    # para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    # definindo os variaveis dentro de linear_decay de acordo com o q-learning\n",
        "    epsilon_linear_decay = linear_decay(initial_E,min_E,X)\n",
        "    t_steps = 0 # contando passos\n",
        "\n",
        "    # loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            epsilon = epsilon_linear_decay(t_steps) # retorna o novo epsilon com base no linear decay e passo atual\n",
        "\n",
        "            # escolhe a próxima ação -- usa epsilon-greedy\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            # realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            if terminated:\n",
        "                # para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # para estados não-terminais -- valor máximo (melhor ação)\n",
        "                V_next_state = np.max(Q[next_state])\n",
        "\n",
        "            # atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state,action]\n",
        "            Q[state,action] = Q[state,action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "            t_steps += 1 # próximo passo\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # a cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i+1) % 100 == 0):\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"Episode {i+1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us3IQDeMx57c"
      },
      "source": [
        "#### Aplicado ao SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7-cvKREx-CF"
      },
      "outputs": [],
      "source": [
        "def run_sarsa_linear_decay(env, episodes, lr=0.1, gamma=0.95, initial_E=1, min_E=0.005, X=1000):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Inicializa a tabela Q toda com zeros\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # Para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    # Definindo as variáveis dentro de linear_decay de acordo com o SARSA\n",
        "    epsilon_linear_decay = linear_decay(initial_E, min_E, X)\n",
        "    t_steps = 0  # Contando passos\n",
        "\n",
        "    # Loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # Escolhe a primeira ação usando epsilon-greedy\n",
        "        epsilon = epsilon_linear_decay(t_steps)\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # Executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # Realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            # Escolhe a próxima ação usando epsilon-greedy\n",
        "            next_epsilon = epsilon_linear_decay(t_steps + 1)\n",
        "            next_action = epsilon_greedy(Q, next_state, next_epsilon)\n",
        "\n",
        "            if terminated:\n",
        "                # Para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # Para estados não-terminais -- valor da próxima ação (SARSA é on-policy)\n",
        "                V_next_state = Q[next_state, next_action]\n",
        "\n",
        "            # Atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state, action]\n",
        "            Q[state, action] = Q[state, action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            action = next_action  # SARSA usa a próxima ação para a atualização\n",
        "\n",
        "            t_steps += 1  # Próximo passo\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # A cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i + 1) % 100 == 0):\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"Episode {i + 1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YBYR3-f9jP3"
      },
      "source": [
        "#### Aplicado ao Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8gxgnos9jP4"
      },
      "outputs": [],
      "source": [
        "def run_expected_sarsa_linear_decay(env, episodes, lr=0.1, gamma=0.95, initial_E=1, min_E=0.005, X=1000):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Inicializa a tabela Q toda com zeros\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # Para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    # Definindo as variáveis dentro de linear_decay de acordo com o Expected-SARSA\n",
        "    epsilon_linear_decay = linear_decay(initial_E, min_E, X)\n",
        "    t_steps = 0  # Contando passos\n",
        "\n",
        "    # Loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # Escolhe a primeira ação usando epsilon-greedy\n",
        "        epsilon = epsilon_linear_decay(t_steps)\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # Executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # Realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            # Calcula o valor esperado para o próximo estado\n",
        "            next_epsilon = epsilon_linear_decay(t_steps + 1)\n",
        "            expected_value = 0  # Inicializa o valor esperado\n",
        "\n",
        "            # Calcula a probabilidade de cada ação no próximo estado\n",
        "            for a in range(num_actions):\n",
        "                if a == np.argmax(Q[next_state, :]):\n",
        "                    # Probabilidade de escolher a ação gulosa (1 - epsilon + epsilon/num_actions)\n",
        "                    prob = (1 - next_epsilon) + (next_epsilon / num_actions)\n",
        "                else:\n",
        "                    # Probabilidade de escolher uma ação aleatória (epsilon/num_actions)\n",
        "                    prob = next_epsilon / num_actions\n",
        "                expected_value += prob * Q[next_state, a]\n",
        "\n",
        "            if terminated:\n",
        "                # Para estados terminais\n",
        "                V_next_state = 0\n",
        "            else:\n",
        "                # Para estados não-terminais -- usa o valor esperado\n",
        "                V_next_state = expected_value\n",
        "\n",
        "            # Atualiza a Q-table\n",
        "            # delta = (estimativa usando a nova recompensa) - estimativa antiga\n",
        "            delta = (reward + gamma * V_next_state) - Q[state, action]\n",
        "            Q[state, action] = Q[state, action] + lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            action = epsilon_greedy(Q, state, next_epsilon)  # Escolhe a próxima ação\n",
        "\n",
        "            t_steps += 1  # Próximo passo\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # A cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i + 1) % 100 == 0):\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"Episode {i + 1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev5B98j6-jxz"
      },
      "source": [
        "#### Aplicado ao SARSA de n Passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hixH5yK2-zxw"
      },
      "outputs": [],
      "source": [
        "def run_nstep_sarsa_linear_decay(env, episodes, n=3, lr=0.1, gamma=0.95, initial_E=1, min_E=0.005, X=1000):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # Inicializa a tabela Q toda com zeros\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))\n",
        "\n",
        "    # Para cada episódio, guarda sua soma de recompensas (retorno não-descontado)\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    # Definindo as variáveis dentro de linear_decay de acordo com o SARSA de n-passos\n",
        "    epsilon_linear_decay = linear_decay(initial_E, min_E, X)\n",
        "    t_steps = 0  # Contando passos\n",
        "\n",
        "    # Loop principal\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards, reward = 0, 0\n",
        "\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        # Escolhe a primeira ação usando epsilon-greedy\n",
        "        epsilon = epsilon_linear_decay(t_steps)\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        # Listas para armazenar os últimos n estados, ações e recompensas\n",
        "        states = [state]\n",
        "        actions = [action]\n",
        "        rewards = [0]  # A primeira recompensa é 0, pois ainda não houve ação\n",
        "\n",
        "        # Executa 1 episódio completo, fazendo atualizações na Q-table\n",
        "        while not done:\n",
        "            # Realiza a ação, ou seja, dá um passo no ambiente\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            # Escolhe a próxima ação usando epsilon-greedy\n",
        "            next_epsilon = epsilon_linear_decay(t_steps + 1)\n",
        "            next_action = epsilon_greedy(Q, next_state, next_epsilon)\n",
        "\n",
        "            # Armazena o próximo estado, ação e recompensa\n",
        "            states.append(next_state)\n",
        "            actions.append(next_action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Se já tivermos n passos, atualizamos a Q-table\n",
        "            if len(states) > n:\n",
        "                # Calcula o retorno de n-passos\n",
        "                G = 0\n",
        "                for j in range(t_steps - n + 1, t_steps + 1):\n",
        "                    G += (gamma ** (j - (t_steps - n))) * rewards[j]\n",
        "\n",
        "                # Adiciona o valor do estado final (se não for terminal)\n",
        "                if not done:\n",
        "                    G += (gamma ** n) * Q[next_state, next_action]\n",
        "\n",
        "                # Atualiza a Q-table para o estado e ação de n passos atrás\n",
        "                update_state = states[t_steps - n]\n",
        "                update_action = actions[t_steps - n]\n",
        "                Q[update_state, update_action] += lr * (G - Q[update_state, update_action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            t_steps += 1  # Próximo passo\n",
        "\n",
        "        # Atualiza os últimos n-1 passos que não foram atualizados\n",
        "        for k in range(max(0, len(states) - n), len(states) - 1):\n",
        "            G = 0\n",
        "            for j in range(k, len(states) - 1):\n",
        "                G += (gamma ** (j - k)) * rewards[j + 1]\n",
        "            if not done:\n",
        "                G += (gamma ** (len(states) - 1 - k)) * Q[states[-1], actions[-1]]\n",
        "            Q[states[k], actions[k]] += lr * (G - Q[states[k], actions[k]])\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # A cada 100 episódios, imprime informação sobre o progresso\n",
        "        if VERBOSE and ((i + 1) % 100 == 0):\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"Episode {i + 1} Average Reward (last 100): {avg_reward:.3f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7-ULM5r0oMa"
      },
      "source": [
        "### Decaimento exponencial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ieb6QSMxC3dG"
      },
      "source": [
        "#### Função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neSxhaZuDCL7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def exponential_decay(initial_E=1.0, min_E=0.005, decay_rate=0.001):\n",
        "    \"\"\"\n",
        "    Função de decaimento exponencial para epsilon.\n",
        "\n",
        "    Parâmetros:\n",
        "        initial_E (float): Valor inicial de epsilon (epsilon máximo).\n",
        "        min_E (float): Valor mínimo de epsilon.\n",
        "        decay_rate (float): Taxa de decaimento (controla a velocidade do decaimento).\n",
        "\n",
        "    Retorna:\n",
        "        Uma função que calcula o epsilon para um dado passo t.\n",
        "    \"\"\"\n",
        "    def new_E(t):\n",
        "        # Decaimento exponencial suave em direção a min_E\n",
        "        return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "\n",
        "    return new_E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4_rJ2LDYtF"
      },
      "source": [
        "#### Aplicado ao Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf4Zyc47DcNb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import gym  # Certifique-se de instalar o Gym com: pip install gym\n",
        "\n",
        "def exponential_decay(initial_E=1.0, min_E=0.005, decay_rate=0.001):\n",
        "    def new_E(t):\n",
        "        return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "    return new_E\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(Q[state]))  # Exploração\n",
        "    else:\n",
        "        best_actions = np.where(Q[state] == np.max(Q[state]))[0]\n",
        "        return np.random.choice(best_actions)  # Escolhe entre as melhores ações\n",
        "\n",
        "def run_qlearning_exponential_decay(env, episodes, lr=0.1, gamma=0.95,\n",
        "                                    initial_E=1.0, min_E=0.005, decay_rate=0.001,\n",
        "                                    verbose=True):\n",
        "    # Garantindo que o ambiente é discreto\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"Apenas espaços discretos são suportados!\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"Apenas ações discretas são suportadas!\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    epsilon_decay = exponential_decay(initial_E, min_E, decay_rate)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()  # Ajuste para Gym v0.26+\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        t_steps = 0  # Reseta a cada episódio\n",
        "\n",
        "        while not done:\n",
        "            epsilon = epsilon_decay(t_steps)\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Atualização Q-Learning\n",
        "            V_next_state = 0 if done else np.max(Q[next_state])\n",
        "            Q[state, action] += lr * (reward + gamma * V_next_state - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            t_steps += 1\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))\n",
        "        epsilon_values.append(epsilon_decay(t_steps))\n",
        "\n",
        "        if verbose and (i + 1) % 100 == 0:\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBroZK5IkYj"
      },
      "source": [
        "#### Aplicado ao SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT2y8cs9IlxW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import gym  # Certifique-se de instalar o Gym com: pip install gym\n",
        "\n",
        "def exponential_decay(initial_E=1.0, min_E=0.005, decay_rate=0.001):\n",
        "    def new_E(t):\n",
        "        return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "    return new_E\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(Q[state]))  # Exploração\n",
        "    else:\n",
        "        best_actions = np.where(Q[state] == np.max(Q[state]))[0]\n",
        "        return np.random.choice(best_actions)  # Escolha aleatória entre as melhores ações\n",
        "\n",
        "def run_sarsa_exponential_decay(env, episodes, lr=0.1, gamma=0.95, initial_E=1.0, min_E=0.005, decay_rate=0.001, verbose=True):\n",
        "    # Garantindo que o ambiente é discreto\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"Apenas espaços discretos são suportados!\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"Apenas ações discretas são suportadas!\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    epsilon_decay = exponential_decay(initial_E, min_E, decay_rate)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()  # Ajuste para Gym v0.26+\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay(i)  # Decaimento baseado no episódio\n",
        "        epsilon_values.append(epsilon)  # Registrar o valor do epsilon\n",
        "\n",
        "        # Escolhe a primeira ação usando epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        while not done:\n",
        "            # Executa a ação no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Escolhe a próxima ação usando epsilon-greedy\n",
        "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
        "\n",
        "            # Atualiza a Q-table (SARSA usa a próxima ação)\n",
        "            V_next_state = 0 if done else Q[next_state, next_action]\n",
        "            Q[state, action] += lr * (reward + gamma * V_next_state - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            action = next_action  # Atualiza a ação para o próximo passo\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))  # Média móvel\n",
        "\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q40W1P-1L3E7"
      },
      "source": [
        "#### Aplicado ao Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U92dDL7_L7OZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import gym  # Certifique-se de instalar com: pip install gym\n",
        "\n",
        "def exponential_decay(initial_E=1.0, min_E=0.005, decay_rate=0.001):\n",
        "    def new_E(t):\n",
        "        return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "    return new_E\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(Q[state]))  # Exploração\n",
        "    else:\n",
        "        best_actions = np.where(Q[state] == np.max(Q[state]))[0]\n",
        "        return np.random.choice(best_actions)  # Escolha aleatória entre as melhores ações\n",
        "\n",
        "def run_expected_sarsa_exponential_decay(env, episodes, lr=0.1, gamma=0.95, initial_E=1.0, min_E=0.005, decay_rate=0.001, verbose=True):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de estados discreto.\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de ações discreto.\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    epsilon_decay = exponential_decay(initial_E, min_E, decay_rate)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()  # Ajuste para Gym v0.26+\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay(i)  # Decaimento baseado no episódio\n",
        "        epsilon_values.append(epsilon)  # Registrar o valor do epsilon\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            next_epsilon = epsilon_decay(i + 1)  # Usando episódio como referência\n",
        "\n",
        "            # Cálculo do valor esperado para o próximo estado\n",
        "            expected_value = 0\n",
        "            best_actions = np.where(Q[next_state] == np.max(Q[next_state]))[0]\n",
        "            prob_best_action = (1 - next_epsilon) + (next_epsilon / num_actions)\n",
        "            prob_other_actions = next_epsilon / num_actions\n",
        "\n",
        "            for a in range(num_actions):\n",
        "                prob = prob_best_action if a in best_actions else prob_other_actions\n",
        "                expected_value += prob * Q[next_state, a]\n",
        "\n",
        "            # Atualização da Q-table (Expected SARSA)\n",
        "            Q[state, action] += lr * (reward + gamma * expected_value - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))  # Média móvel\n",
        "\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFfo3CcmPcTr"
      },
      "source": [
        "#### Aplicado ao SARSA de N-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ay5efJePgza"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import gym  # Certifique-se de instalar com: pip install gym\n",
        "\n",
        "def exponential_decay(initial_E=1.0, min_E=0.005, decay_rate=0.001):\n",
        "    \"\"\"Função para calcular epsilon com decaimento exponencial.\"\"\"\n",
        "    def new_E(t):\n",
        "        return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "    return new_E\n",
        "\n",
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    \"\"\"Política epsilon-greedy para escolher ações.\"\"\"\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(len(Q[state]))  # Exploração\n",
        "    else:\n",
        "        best_actions = np.where(Q[state] == np.max(Q[state]))[0]\n",
        "        return np.random.choice(best_actions)  # Escolhe entre as ações ótimas\n",
        "\n",
        "def run_nstep_sarsa_exponential_decay(env, episodes, n=3, lr=0.1, gamma=0.95, initial_E=1.0, min_E=0.005, decay_rate=0.001, verbose=True):\n",
        "    \"\"\"Executa n-step SARSA com decaimento exponencial de epsilon.\"\"\"\n",
        "\n",
        "    # Verificações do ambiente\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de estados discreto.\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de ações discreto.\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))  # Inicializa a Q-table\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    epsilon_decay = exponential_decay(initial_E, min_E, decay_rate)  # Função de decaimento de epsilon\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()  # Ajuste para Gym v0.26+\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay(i)  # Epsilon atualizado por episódio\n",
        "\n",
        "        # Armazena as últimas transições para atualização n-step\n",
        "        states = [state]\n",
        "        actions = [epsilon_greedy(Q, state, epsilon)]\n",
        "        rewards = [0]  # Inicialmente zero\n",
        "\n",
        "        step_count = 0\n",
        "        while not done:\n",
        "            # Executa ação no ambiente\n",
        "            next_state, reward, terminated, truncated, _ = env.step(actions[-1])\n",
        "            done = terminated or truncated\n",
        "\n",
        "            sum_rewards += reward\n",
        "            states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Se não for terminal, escolhe a próxima ação\n",
        "            if not done:\n",
        "                next_epsilon = epsilon_decay(i)  # Atualiza epsilon com base no episódio\n",
        "                actions.append(epsilon_greedy(Q, next_state, next_epsilon))\n",
        "            else:\n",
        "                actions.append(None)  # No terminal, não há ação\n",
        "\n",
        "            # Atualiza a Q-table usando o retorno de n-passos\n",
        "            if step_count >= n - 1:\n",
        "                G = sum(gamma ** j * rewards[j] for j in range(1, n + 1))  # Soma das recompensas futuras\n",
        "\n",
        "                if not done and actions[-1] is not None:  # Se não for terminal, adiciona valor esperado\n",
        "                    G += (gamma ** n) * Q[states[-1], actions[-1]]\n",
        "\n",
        "                Q[states[0], actions[0]] += lr * (G - Q[states[0], actions[0]])\n",
        "\n",
        "                # Mantém o buffer de tamanho `n`\n",
        "                del states[0]\n",
        "                del actions[0]\n",
        "                del rewards[0]\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # Atualiza os últimos passos ao final do episódio\n",
        "        while len(states) > 1:  # Atualiza todas as transições restantes\n",
        "            G = sum(gamma ** j * rewards[j] for j in range(1, len(states)))\n",
        "            if actions[-1] is not None:\n",
        "                G += (gamma ** (len(states) - 1)) * Q[states[-1], actions[-1]]\n",
        "\n",
        "            Q[states[0], actions[0]] += lr * (G - Q[states[0], actions[0]])\n",
        "\n",
        "            del states[0]\n",
        "            del actions[0]\n",
        "            del rewards[0]\n",
        "\n",
        "        # Exibe progresso a cada 100 episódios\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_reward:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfieBxUT0Qpq"
      },
      "source": [
        "## Decaimento Adaptativo do ε"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t7pTkbs07GJ"
      },
      "source": [
        "### Decaimento baseado no desempenho recente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CibLZCqMbeCZ"
      },
      "source": [
        "#### Função"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DENjikAxb0CZ"
      },
      "source": [
        "- Se o desempenho médio dos últimos X episódios não piorar, reduzir ε.\n",
        "- Se o desempenho cair, manter ε estável ou aumentar levemente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uaVUOMtbwVR"
      },
      "outputs": [],
      "source": [
        "def adaptive_decay_E(E, recent_rewards, threshold=0.005, decay_factor=0.995, increase_factor=1.005, min_E=0.005, max_E=0.995, X=200):\n",
        "    if len(recent_rewards) < 2:\n",
        "        return E  # Não há dados suficientes para avaliar\n",
        "\n",
        "    N = min(50, len(recent_rewards)//2)  # Número de episódios considerados\n",
        "    recent_mean = np.mean(recent_rewards[-N:])\n",
        "\n",
        "    if len(recent_rewards) >= 2*N:\n",
        "        previous_mean = np.mean(recent_rewards[-2*N:-N])\n",
        "    else:\n",
        "        previous_mean = recent_mean  # Evita erro de variável não inicializada\n",
        "\n",
        "    if recent_mean >= (previous_mean - threshold):  # Melhorou ou manteve\n",
        "        E = max(E * decay_factor, min_E)\n",
        "    else:  # Desempenho piorou\n",
        "        if E < max_E * 0.9:  # Permite aumento apenas se estiver significativamente abaixo do máximo\n",
        "            E = min(E * increase_factor, max_E)\n",
        "\n",
        "    return E\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8CBhgLkbrv0"
      },
      "source": [
        "#### Aplicado ao Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z8KRT2_bsil"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def run_qlearning_adaptive(env, episodes, lr=0.1, gamma=0.95, initial_E=.995, min_E=0.005, threshold=0.005, decay_factor=0.995, increase_factor=1.005, X = 200): # X = numero X de episodios para atualizar\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))  # Inicializa a tabela Q\n",
        "\n",
        "    all_episode_rewards = []\n",
        "    recent_rewards = []\n",
        "    epsilon = initial_E\n",
        "\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            # Atualiza Q-table (Q-Learning)\n",
        "            V_next_state = 0 if terminated else np.max(Q[next_state])\n",
        "            Q[state, action] += lr * ((reward + gamma * V_next_state) - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        recent_rewards.append(sum_rewards)\n",
        "\n",
        "        if len(recent_rewards) > X:\n",
        "            recent_rewards.pop(0)\n",
        "\n",
        "        epsilon = adaptive_decay_E(epsilon, recent_rewards, threshold, decay_factor, increase_factor, min_E,X)\n",
        "\n",
        "        if (i + 1) % X == 0:\n",
        "            avg_reward = np.mean(all_episode_rewards[-X:])\n",
        "            print(f\"Episode {i+1} | Avg Reward: {avg_reward:.3f} | Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hD42QfGbWaZ"
      },
      "source": [
        "#### Aplicado ao SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INH5_iDNiJYN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqyfviioh1t-"
      },
      "source": [
        "#### Aplicado ao Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6JG68cKiH30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cux5zoTKiMAu"
      },
      "source": [
        "#### Aplicado ao SARSA de n-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KfiA5sPiKaS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syGwwpiv0-4p"
      },
      "source": [
        "### Decaimento proporcional ao desempenho greedy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVY8J55F2ZZ4"
      },
      "source": [
        "- Definir um desempenho alvo baseado na melhor política conhecida.\n",
        "- Ajustar ε proporcionalmente à distância entre o desempenho atual e o desempenho alvo.\n",
        "\n",
        "Edtratégia:\n",
        "- Se a recompensa atual estiver abaixo do alvo -> aumenta epsilon (explorar mais)\n",
        "-Se estiver próximo/acima do alvo -> reduz epsilon (explorar menos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_5KyVOTieHm"
      },
      "source": [
        "#### Função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxbhu0hsidWI"
      },
      "outputs": [],
      "source": [
        "def adaptive_decay_E_target(E, current_reward,best_avg_reward, decay_factor =0.99, increase_factor=1.01, min_E=.01, max_E-1.00):\n",
        "\n",
        "  if best_avg_reward is None:\n",
        "    return E\n",
        "  performance_ratio = current_reward / best_avg_reward # proporção de proximidade\n",
        "\n",
        "  if performance_ratio < 1.0:\n",
        "    E *= increase_factor\n",
        "  else:\n",
        "    E *= decay_factor\n",
        "\n",
        "  return max(min_E, min(E,max_E)) # E nunca passa de máximo ou mínimo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbMjABaVkivl"
      },
      "source": [
        "#### Aplicado ao Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Set7hqECkmAj"
      },
      "outputs": [],
      "source": [
        "# Algoritmo Q-learning com decaimento proporcional ao desempenho alvo\n",
        "def run_qlearning_target(env, episodes, lr=0.1, gamma=0.95, initial_E=1.0, min_E=0.01, max_E=1.0, decay_factor=0.99, increase_factor=1.01):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros(shape=(env.observation_space.n, num_actions))  # Inicializa a tabela Q\n",
        "\n",
        "    all_episode_rewards = []\n",
        "    epsilon = initial_E\n",
        "    best_avg_reward = None  # melhor desempenho médio - alvo escolhido na estratégia\n",
        "\n",
        "    for i in range(episodes):\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        state, _ = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q, state, epsilon) # E-greedy\n",
        "\n",
        "            next_state, reward, terminated, trunc, _ = env.step(action)\n",
        "            done = terminated or trunc\n",
        "\n",
        "            V_next_state = 0 if terminated else np.max(Q[next_state])\n",
        "            delta = (reward + gamma * V_next_state) - Q[state, action]\n",
        "            Q[state, action] += lr * delta\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "\n",
        "        # pegando a melhor media a cada 100 episódios - melhorar para X episódios\n",
        "        if (i + 1) % 100 == 0:\n",
        "            best_avg_reward = np.mean(all_episode_rewards[-100:])\n",
        "\n",
        "        # trocando E para um baseado no desempenho alvo\n",
        "        epsilon = adaptive_decay_target(epsilon, sum_rewards, best_avg_reward, decay_factor, increase_factor, min_E, max_E)\n",
        "\n",
        "        if VERBOSE and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i+1} | Avg Reward (last 100): {best_avg_reward:.3f} | Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvausVvN3q9F"
      },
      "source": [
        "## Atualizações - ajuste dinâmico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIqdo2tI4LLe"
      },
      "source": [
        "- Manter um histórico das últimas recompensas médias.\n",
        "- Ajustar ε dependendo da estabilidade do desempenho."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRpQH1yC1VOL"
      },
      "source": [
        "# Testes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fd9q0S-4h-H"
      },
      "source": [
        "Definir melhores ambientes para os decaimentos e parametrização. Criar comparações para o artigo\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCDsb9CqfmIU"
      },
      "source": [
        "## Linear Decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13bt0Fm9fmIW"
      },
      "source": [
        "Q-Learning (Epsilon-greedy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnv15eqTfmIX"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 8_000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.05\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "# Roda o algoritmo Q-Learning\n",
        "rewards1, qtable1 = run_qlearning_linear_decay(env, EPISODES, LR, GAMMA)\n",
        "clear_output()\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTmjvwVKfmIY"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards1, r_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z1ntbHw8qTg"
      },
      "source": [
        "## Adaptive Decay 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymKgRJw28uQb"
      },
      "source": [
        "Q-Learning (Epsilon-greedy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zrZZBnr8yZe"
      },
      "outputs": [],
      "source": [
        "#ENV_NAME, r_max = \"Taxi-v3\", 10\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "\n",
        "EPISODES = 8_000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON = 0.05\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "# Roda o algoritmo Q-Learning\n",
        "rewards1, qtable1 = run_qlearning_adaptive(env, EPISODES, LR, GAMMA)\n",
        "# clear_output()\n",
        "\n",
        "print(\"Últimos resultados: media =\", np.mean(rewards1[-20:]), \", desvio padrao =\", np.std(rewards1[-20:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_iDj56p9B7f"
      },
      "outputs": [],
      "source": [
        "# Mostra um gráfico de episódios x retornos não descontados\n",
        "plot_result(rewards1, r_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATSewAohTzJS"
      },
      "source": [
        "# Experimentos (Frozen, Taxi, Cliff, Racetrack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwJjwpPbZZbP"
      },
      "source": [
        "## Setup inicial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1NXGIsRrY4Nr"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import sys\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    # for saving videos\n",
        "    !apt-get install ffmpeg\n",
        "\n",
        "    !pip install gymnasium moviepy\n",
        "    !pip install optuna\n",
        "\n",
        "    # clone repository\n",
        "    !git clone https://github.com/pablo-sampaio/rl_facil\n",
        "    sys.path.append(\"/content/rl_facil\")\n",
        "\n",
        "    clear_output()\n",
        "else:\n",
        "    from os import path\n",
        "    sys.path.append( path.dirname( path.dirname( path.abspath(\"__main__\") ) ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "1H5DyqxUZCQO"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from functools import partial\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import math\n",
        "from util.experiments import repeated_exec\n",
        "from util.plot import plot_result, plot_multiple_results\n",
        "import matplotlib.pyplot as plt\n",
        "from util.notebook import display_videos_from_path\n",
        "from util.qtable_helper import evaluate_qtable_policy, record_video_qtable\n",
        "# define se os algoritmos vão imprimir dados do treinamento\n",
        "VERBOSE = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "WDCgtMW5ZVKn"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(Q, state, epsilon):\n",
        "    num_actions = len(Q[state])\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.random.randint(0, num_actions)\n",
        "    else:\n",
        "        return np.argmax(Q[state])   # em caso de empates, retorna sempre o menor índice --> mais eficiente, porém...\n",
        "        #return np.random.choice(np.where(Q[state] == Q[state].max())[0]) # aleatoriza em caso de empates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BW7tZMXX0nj"
      },
      "source": [
        "## Decaimento parametrizado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "re_lAZ_uXzuy"
      },
      "outputs": [],
      "source": [
        "def epsilon_decay(decay_type='none', initial_E=1.0, min_E=0.005, X=1000, Y=None, total_steps=1000,episodes=8000, decay_rate=0.001):\n",
        "    \"\"\"\n",
        "        decay_type (str): Tipo de decaimento ('linear' ou 'exponential', ou 'none').\n",
        "        initial_E (float): Valor inicial de epsilon (epsilon máximo).\n",
        "        min_E (float): Valor mínimo de epsilon.\n",
        "        X (int): Número de passos para atingir o epsilon mínimo (opcional).\n",
        "        Y (float): Percentual do tempo total de treinamento para decaimento (opcional).\n",
        "        total_steps (int): Número total de passos de treinamento.\n",
        "        decay_rate (float): Taxa de decaimento (apenas para decaimento exponencial).\n",
        "    \"\"\"\n",
        "    if X is None and Y is None:\n",
        "        raise ValueError(\"Deve inserir X (número de passos) ou Y (percentual do tempo total).\")\n",
        "\n",
        "\n",
        "\n",
        "    if Y is not None:\n",
        "        if total_steps is None:\n",
        "          estimated_steps_per_episode = 500 # sendo usado no env\n",
        "          total_steps = episodes * estimated_steps_per_episode\n",
        "        X = int(Y * total_steps)\n",
        "\n",
        "    if decay_type == 'none':\n",
        "        def new_E(t):\n",
        "            return initial_E\n",
        "    elif decay_type == 'linear':\n",
        "        k = (initial_E - min_E) / X\n",
        "        def new_E(t):\n",
        "            return max(min_E, initial_E - k * t)\n",
        "    elif decay_type == 'exponential':\n",
        "        def new_E(t):\n",
        "            return min_E + (initial_E - min_E) * math.exp(-decay_rate * t)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Tipo de decaimento inválido. Escolha 'linear' ou 'exponential'.\")\n",
        "\n",
        "    return new_E"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izPcSmF6a7yD"
      },
      "source": [
        "## Algoritmos TD-learning adaptados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5RK_IiSgOqE"
      },
      "source": [
        "#### Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "4mahLLVabBP5"
      },
      "outputs": [],
      "source": [
        "def run_qlearning_decay(env, episodes, lr=0.1, gamma=0.95, decay_type='linear', initial_E=1.0, min_E=0.005, X=None, Y=None, total_steps=None, decay_rate=0.001, verbose=True, n_eps_per_rewards=500):\n",
        "    \"\"\"\n",
        "    Parâmetros:\n",
        "        env: Ambiente Gym.\n",
        "        episodes (int): Número de episódios de treinamento.\n",
        "        lr (float): Taxa de aprendizado (learning rate).\n",
        "        gamma (float): Fator de desconto.\n",
        "        decay_type (str): Tipo de decaimento ('linear' ou 'exponential').\n",
        "        initial_E (float): Valor inicial de epsilon.\n",
        "        min_E (float): Valor mínimo de epsilon.\n",
        "        X (int): Número de passos para atingir o epsilon mínimo (opcional).\n",
        "        Y (float): Percentual do tempo total de treinamento para decaimento (opcional).\n",
        "        total_steps (int): Número total de passos de treinamento (necessário se Y for fornecido).\n",
        "        decay_rate (float): Taxa de decaimento (apenas para decaimento exponencial).\n",
        "        verbose (bool): Se True, imprime o progresso durante o treinamento.\n",
        "\n",
        "    Retorna:\n",
        "        all_episode_rewards (list): Recompensas de todos os episódios.\n",
        "        Q (np.array): Tabela Q aprendida.\n",
        "        avg_rewards (list): Média das recompensas dos últimos 100 episódios.\n",
        "        epsilon_values (list): Valores de epsilon ao longo do treinamento.\n",
        "    \"\"\"\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"Apenas espaços discretos são suportados!\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"Apenas ações discretas são suportadas!\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    epsilon_decay_fn = epsilon_decay(decay_type=decay_type, initial_E=initial_E, min_E=min_E, Y=Y, total_steps=total_steps, decay_rate=decay_rate,episodes=episodes)\n",
        "\n",
        "    eps = 0\n",
        "    for i in range(episodes):\n",
        "        eps += 1\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "\n",
        "\n",
        "        while not done:\n",
        "            epsilon = epsilon_decay_fn(eps)\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Atualização Q-Learning\n",
        "            V_next_state = 0 if done else np.max(Q[next_state])\n",
        "            Q[state, action] += lr * (reward + gamma * V_next_state - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-n_eps_per_rewards:]))\n",
        "        epsilon_values.append(epsilon)\n",
        "        if verbose and (i + 1) % n_eps_per_rewards == 0:\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last {n_eps_per_rewards}): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeAUDiBSgMAU"
      },
      "source": [
        "#### SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "_l2it4CUe13n"
      },
      "outputs": [],
      "source": [
        "def run_sarsa_decay(env, episodes, lr=0.1, gamma=0.95, decay_type='linear', initial_E=1.0, min_E=0.005, X=None, Y=None, total_steps=1000, decay_rate=0.001, verbose=True):\n",
        "    # Garantindo que o ambiente é discreto\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"Apenas espaços discretos são suportados!\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"Apenas ações discretas são suportadas!\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    # Criar a função de decaimento do epsilon\n",
        "    epsilon_decay_fn = epsilon_decay(decay_type=decay_type, initial_E=initial_E, min_E=min_E, Y=Y, total_steps=total_steps, decay_rate=decay_rate)\n",
        "\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay_fn(i)  # Decaimento baseado no episódio\n",
        "        epsilon_values.append(epsilon)  # Registrar o valor do epsilon\n",
        "\n",
        "        # Escolhe a primeira ação usando epsilon-greedy\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        while not done:\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Escolhe a próxima ação usando epsilon-greedy\n",
        "            next_action = epsilon_greedy(Q, next_state, epsilon)\n",
        "\n",
        "            # Atualiza a Q-table (SARSA usa a próxima ação)\n",
        "            V_next_state = 0 if done else Q[next_state, next_action]\n",
        "            Q[state, action] += lr * (reward + gamma * V_next_state - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "            action = next_action  # Atualiza a ação para o próximo passo\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))  # Média móvel\n",
        "\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqayEIcpgGld"
      },
      "source": [
        "#### Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4L5TCbUKgCvI"
      },
      "outputs": [],
      "source": [
        "def run_expected_sarsa_decay(env, episodes, lr=0.1, gamma=0.95, decay_type='exponential', initial_E=1.0, min_E=0.005, X=None, Y=None, total_steps=1000, decay_rate=0.001, verbose=True):\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de estados discreto.\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de ações discreto.\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    epsilon_decay_fn = epsilon_decay(decay_type=decay_type, initial_E=initial_E, min_E=min_E, Y=Y, total_steps=total_steps, decay_rate=decay_rate)\n",
        "\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()  # Ajuste para Gym v0.26+\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay_fn(i)\n",
        "        epsilon_values.append(epsilon)\n",
        "\n",
        "        while not done:\n",
        "            action = epsilon_greedy(Q, state, epsilon)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            next_epsilon = epsilon_decay_fn(i + 1)\n",
        "\n",
        "            expected_value = 0\n",
        "            best_actions = np.where(Q[next_state] == np.max(Q[next_state]))[0]\n",
        "            prob_best_action = (1 - next_epsilon) + (next_epsilon / num_actions)\n",
        "            prob_other_actions = next_epsilon / num_actions\n",
        "\n",
        "            for a in range(num_actions):\n",
        "                prob = prob_best_action if a in best_actions else prob_other_actions\n",
        "                expected_value += prob * Q[next_state, a]\n",
        "\n",
        "            Q[state, action] += lr * (reward + gamma * expected_value - Q[state, action])\n",
        "\n",
        "            sum_rewards += reward\n",
        "            state = next_state\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))\n",
        "\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn8K7_bEiVqc"
      },
      "source": [
        "#### SARSA de n-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-v4v38UmiZWs"
      },
      "outputs": [],
      "source": [
        "def run_nstep_sarsa_decay(env, episodes, n=3, lr=0.1, gamma=0.95, decay_type='linear', initial_E=1.0, min_E=0.005,\n",
        "                     X=None, Y=None, total_steps=1000, decay_rate=0.001, verbose=True):\n",
        "    \"\"\"Executa n-step SARSA com decaimento de epsilon.\"\"\"\n",
        "\n",
        "    assert isinstance(env.observation_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de estados discreto.\"\n",
        "    assert isinstance(env.action_space, gym.spaces.Discrete), \"O ambiente deve ter um espaço de ações discreto.\"\n",
        "\n",
        "    num_actions = env.action_space.n\n",
        "    Q = np.zeros((env.observation_space.n, num_actions))  # Inicializa a Q-table\n",
        "    all_episode_rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilon_values = []\n",
        "\n",
        "    # Criar a função de decaimento do epsilon\n",
        "    epsilon_decay_fn = epsilon_decay(decay_type=decay_type, initial_E=initial_E, min_E=min_E, Y=Y, total_steps=total_steps, decay_rate=decay_rate)\n",
        "\n",
        "    for i in range(episodes):\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        sum_rewards = 0\n",
        "        epsilon = epsilon_decay_fn(i)  # Epsilon atualizado por episódio\n",
        "        epsilon_values.append(epsilon)\n",
        "\n",
        "        # Buffers para armazenar transições\n",
        "        states = [state]\n",
        "        actions = [epsilon_greedy(Q, state, epsilon)]\n",
        "        rewards = [0]  # Inicialmente zero\n",
        "\n",
        "        step_count = 0\n",
        "        while not done:\n",
        "            next_state, reward, terminated, truncated, _ = env.step(actions[-1])\n",
        "            done = terminated or truncated\n",
        "\n",
        "            sum_rewards += reward\n",
        "            states.append(next_state)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if not done:\n",
        "                next_action = epsilon_greedy(Q, next_state, epsilon)\n",
        "                actions.append(next_action)\n",
        "            else:\n",
        "                actions.append(None)\n",
        "\n",
        "            # Atualiza a Q-table usando o retorno de n-passos\n",
        "            if step_count >= n - 1:\n",
        "                G = sum(gamma ** j * rewards[j] for j in range(1, n + 1))  # Soma das recompensas futuras\n",
        "\n",
        "                if not done:\n",
        "                    G += (gamma ** n) * Q[states[-1], actions[-1]]\n",
        "\n",
        "                Q[states[0], actions[0]] += lr * (G - Q[states[0], actions[0]])\n",
        "\n",
        "                # Mantém o buffer de tamanho `n`\n",
        "                del states[0]\n",
        "                del actions[0]\n",
        "                del rewards[0]\n",
        "\n",
        "            step_count += 1\n",
        "\n",
        "        all_episode_rewards.append(sum_rewards)\n",
        "        avg_rewards.append(np.mean(all_episode_rewards[-100:]))\n",
        "\n",
        "        # Atualiza os últimos passos ao final do episódio\n",
        "        while len(states) > 1:\n",
        "            G = sum(gamma ** j * rewards[j] for j in range(1, len(states)))\n",
        "            if actions[-1] is not None:\n",
        "                G += (gamma ** (len(states) - 1)) * Q[states[-1], actions[-1]]\n",
        "\n",
        "            Q[states[0], actions[0]] += lr * (G - Q[states[0], actions[0]])\n",
        "\n",
        "            del states[0]\n",
        "            del actions[0]\n",
        "            del rewards[0]\n",
        "\n",
        "        # Exibe progresso a cada 100 episódios\n",
        "        if verbose and ((i + 1) % 100 == 0):\n",
        "            print(f\"Episode {i + 1}: Avg Reward (last 100): {avg_rewards[-1]:.3f}, Epsilon: {epsilon:.4f}\")\n",
        "\n",
        "    return all_episode_rewards, Q, avg_rewards, epsilon_values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmYTdnulg6dL"
      },
      "source": [
        "## Configuração dos Experimentos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9sybe41m_jx"
      },
      "source": [
        "### Hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOFcwHKzoaAm"
      },
      "outputs": [],
      "source": [
        "# episodios_de_busca = 8000\n",
        "\n",
        "# def train_qlearning_decay(ambient, trial: optuna.Trial):\n",
        "#     eps = trial.suggest_float('initial_E', 1.0, 1.0)\n",
        "#     Y = trial.suggest_float('Y', 0.00001,0.001)\n",
        "#     min_eps = trial.suggest_float('min_E', 0.005, 0.2)\n",
        "#     episodes = trial.suggest_float('episodes', 8000)\n",
        "#     gamma = trial.suggest_float('gamma', 0.90, 1.00)\n",
        "#     lr = trial.suggest_float('lr', 0.001, 1.0, log=True)\n",
        "#     decay_rate = trial.suggest_float('decay_rate', .0001, 0.01, log=True)\n",
        "#     decay_type = trial.suggest_categorical('decay_type', ['linear', 'exponential','none'])\n",
        "\n",
        "#     env = gym.make(ambient)\n",
        "#     returns, _,_,_ = run_qlearning_decay(env, episodios_de_busca, lr=lr, gamma=gamma, decay_type=decay_type,\n",
        "#                                initial_E=eps, min_E=min_eps, decay_rate=decay_rate,Y=Y)\n",
        "\n",
        "#     return sum(returns[-100:]) / 100\n",
        "\n",
        "# def train_sarsa_decay(ambient, trial: optuna.Trial):\n",
        "#     decay_rate = trial.suggest_float('decay_rate', .0001, 0.01, log=True)\n",
        "\n",
        "#     eps = trial.suggest_float('initial_E', 1.0, 1.0)  # Epsilon inicial\n",
        "#     Y = trial.suggest_float('Y', 0.00001,0.001)\n",
        "#     min_eps = trial.suggest_float('min_E', 0.005, 0.2)\n",
        "#     gamma = trial.suggest_float('gamma', 0.90, 1.00)  # Fator de desconto\n",
        "#     lr = trial.suggest_float('lr', 0.001, 1.0, log=True)  # Taxa de aprendizado\n",
        "#     decay_rate = trial.suggest_float('decay_rate', .0001, 0.01, log=True)\n",
        "#     decay_type = trial.suggest_categorical('decay_type', ['linear', 'exponential','none'])\n",
        "#     episodes = trial.suggest_float('episodes', 8000)\n",
        "\n",
        "#     env = gym.make(ambient)\n",
        "#     returns, _,_,_ = run_sarsa_decay(env, episodios_de_busca, lr=lr, gamma=gamma, decay_type=decay_type,\n",
        "#                            initial_E=eps, min_E=min_eps, decay_rate=decay_rate)\n",
        "\n",
        "#     return sum(returns[-100:]) / 100\n",
        "\n",
        "# def train_expected_sarsa_decay(ambient, trial: optuna.Trial):\n",
        "#     Y = trial.suggest_float('Y', 0.0001,0.01)\n",
        "#     eps = trial.suggest_float('initial_E', 1.0, 1.0)\n",
        "#     min_eps = trial.suggest_float('min_E', 0.005, 0.2)\n",
        "#     gamma = trial.suggest_float('gamma', 0.90, 1.00)\n",
        "#     lr = trial.suggest_float('lr', 0.001, 1.0, log=True)\n",
        "#     decay_rate = trial.suggest_float('decay_rate', .0001, 0.01, log=True)\n",
        "#     decay_type = trial.suggest_categorical('decay_type', ['linear', 'exponential','none'])\n",
        "#     episodes = trial.suggest_float('episodes', 8000)\n",
        "\n",
        "#     env = gym.make(ambient)\n",
        "#     returns, _,_,_ = run_expected_sarsa_decay(env, episodios_de_busca, lr=lr, gamma=gamma, decay_type=decay_type,\n",
        "#                                     initial_E=eps, min_E=min_eps, decay_rate=decay_rate)\n",
        "\n",
        "#     return sum(returns[-100:]) / 100\n",
        "\n",
        "# def train_nstep_sarsa_decay(ambient, trial: optuna.Trial):\n",
        "#     decay_rate = trial.suggest_float('decay_rate', .0001, 0.01, log=True)\n",
        "#     Y = trial.suggest_float('Y', 0.0001,0.01)\n",
        "#     eps = trial.suggest_float('initial_E', 1.0, 1.0)\n",
        "#     min_eps = trial.suggest_float('min_E', 0.005, 0.2)\n",
        "#     gamma = trial.suggest_float('gamma', 0.90, 1.00)\n",
        "#     lr = trial.suggest_float('lr', 0.001, 1.0, log=True)\n",
        "#     decay_type = trial.suggest_categorical('decay_type', ['linear', 'exponential','none'])\n",
        "#     nsteps = trial.suggest_int('nsteps', 3, 3) # 1-16\n",
        "#     episodes = trial.suggest_float('episodes', 8000)\n",
        "\n",
        "#     env = gym.make(ambient)\n",
        "#     returns, _,_,_ = run_nstep_sarsa_decay(env, episodios_de_busca, n=nsteps, lr=lr, gamma=gamma, decay_type=decay_type,\n",
        "#                                  initial_E=eps, min_E=min_eps, decay_rate=decay_rate)\n",
        "\n",
        "#     return sum(returns[-100:]) / 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCoZ48LCrZhY"
      },
      "source": [
        "#### Buscando para cada ambiente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itM8LWO2rY-z"
      },
      "outputs": [],
      "source": [
        "# ambientes = [\"CliffWalking-v0\", \"Taxi-v3\", \"FrozenLake-v1\",\"RaceTrack-v0\"]\n",
        "\n",
        "\n",
        "# best_params = {}\n",
        "# trials = 1 # 50\n",
        "\n",
        "# for ambient in ambientes:\n",
        "#     print(f\"\\n🔹 Otimizando para ambiente: {ambient}\")\n",
        "\n",
        "#     best_params[ambient] = {}\n",
        "\n",
        "#     study_qlearning = optuna.create_study(direction='maximize')\n",
        "#     study_qlearning.optimize(partial(train_qlearning_decay, ambient), n_trials=trials)\n",
        "#     best_params[ambient][\"Q-Learning\"] = study_qlearning.best_params\n",
        "#     print(f\"Melhores parâmetros para Q-Learning em {ambient}: {study_qlearning.best_params}\")\n",
        "\n",
        "#     study_sarsa = optuna.create_study(direction='maximize')\n",
        "#     study_sarsa.optimize(partial(train_sarsa_decay, ambient), n_trials=trials)\n",
        "#     best_params[ambient][\"SARSA\"] = study_sarsa.best_params\n",
        "#     print(f\"Melhores parâmetros para SARSA em {ambient}: {study_sarsa.best_params}\")\n",
        "\n",
        "#     study_expected_sarsa = optuna.create_study(direction='maximize')\n",
        "#     study_expected_sarsa.optimize(partial(train_expected_sarsa_decay, ambient), n_trials=trials)\n",
        "#     best_params[ambient][\"Expected SARSA\"] = study_expected_sarsa.best_params\n",
        "#     print(f\"Melhores parâmetros para Expected SARSA em {ambient}: {study_expected_sarsa.best_params}\")\n",
        "\n",
        "#     study_nstep_sarsa = optuna.create_study(direction='maximize')\n",
        "#     study_nstep_sarsa.optimize(partial(train_nstep_sarsa_decay, ambient), n_trials=trials)\n",
        "#     best_params[ambient][\"n-step SARSA\"] = study_nstep_sarsa.best_params\n",
        "#     print(f\"Melhores parâmetros para n-step SARSA em {ambient}: {study_nstep_sarsa.best_params}\")\n",
        "\n",
        "# clear_output()\n",
        "# print(\"\\n Melhores hiperparâmetros para todos os ambientes e algoritmos:\")\n",
        "# for ambient, algos in best_params.items():\n",
        "#     print(f\"\\n🔹 Ambiente: {ambient}\")\n",
        "#     for algo, params in algos.items():\n",
        "#         print(f\"  🔸 {algo}: {params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyq7TL0A74Tk"
      },
      "source": [
        "#### Funções de plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "DaWnYl2L72zr"
      },
      "outputs": [],
      "source": [
        "def plot_results(title,rewards_list, labels):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for rewards, label in zip(rewards_list, labels):\n",
        "        plt.plot(np.convolve(rewards, np.ones(10)/10, mode='valid'), label=label, linewidth=2)\n",
        "\n",
        "    plt.xlabel(\"Episódios\")\n",
        "    plt.ylabel(\"Recompensa média a cada 10 ep.\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEmNoFJuVjlT"
      },
      "source": [
        "### Cliffwalking - Tentando vencer o Epsilon Fixo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vJp9xs5VjlU"
      },
      "source": [
        "##### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "4paEw8ubVjlU"
      },
      "outputs": [],
      "source": [
        "# ENV_NAME = \"CliffWalking-v0\"\n",
        "# EPISODES = 8000\n",
        "# LR = 0.01\n",
        "# GAMMA = 0.95\n",
        "# EPSILON_NORMAL = .005\n",
        "# EPSILON_START = 1.0\n",
        "# EPSILON_MIN = 0.005\n",
        "# DECAY_LINEAR = .0000000001\n",
        "# DECAY_EXP = .0001\n",
        "# Y_LINEAR=.1\n",
        "# Y_EXP = 0.000001\n",
        "# X_LINEAR=500\n",
        "# X_EXP =8000\n",
        "N_EPS_PER_REWARDS=200\n",
        "\n",
        "#######\n",
        "ENV_NAME, r_max = \"CliffWalking-v0\", 0\n",
        "EPISODES = 8_000\n",
        "LR = 0.2\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = 0.1\n",
        "\n",
        "# Verificar essas var\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "#######\n",
        "# Y_LINEAR=.1\n",
        "# Y_EXP = 0.000001\n",
        "# X_LINEAR=500\n",
        "# X_EXP =8000\n",
        "#######\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR, n_eps_per_rewards=N_EPS_PER_REWARDS)\n",
        "rewards_exponential, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP,n_eps_per_rewards=N_EPS_PER_REWARDS)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_qlearning_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0,n_eps_per_rewards=N_EPS_PER_REWARDS  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "wrtDUGKpVjlU",
        "outputId": "e30c5edf-6007-4dff-91a2-c0b122451770"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_results(\"Q-Learning (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear,rewards_exponential], [\"Fixo\",\"Linear\",\"Exponencial\"])\n",
        "#plot_results(\"Q-Learning (CliffWalking) - Fixo x Linear\",[rewards_no_decay,rewards_linear], [\"Fixo\",\"Linear\"])\n",
        "\n",
        "# plot_results(\"Q-Learning (CliffWalking) - Fixo\",[rewards_no_decay], [\"Fixo\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bztpHOtcVjlU"
      },
      "source": [
        "##### Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZQWrBawVjlV"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_sarsa_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvW83NYTVjlV"
      },
      "outputs": [],
      "source": [
        "plot_results(\"SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcy6K2HqVjlV"
      },
      "source": [
        "##### Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU_ndjGlVjlV"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR-S09KoVjlV"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Expected-SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtmZjOo-VjlW"
      },
      "source": [
        "##### SARSA de n-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "953uZ1oRVjlW"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NjhdzzMVjlW"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Sarsa de n-passos (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-lYBrIzVmPf"
      },
      "source": [
        "### Cliffwalking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDlGwtSAVmPh"
      },
      "outputs": [],
      "source": [
        "# plot_results(\"Linear x Exponencial (Q-Learning) - CliffWalking\",[rewards_linear, rewards_exponential], [\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnCZXR51VmPi"
      },
      "source": [
        "##### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkvToMenVmPi"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_qlearning_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4wIbe-GVmPj"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Q-Learning (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBZ6dLbqVmPk"
      },
      "source": [
        "##### Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFsLZAMDVmPl"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_sarsa_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0wFNairVmPl"
      },
      "outputs": [],
      "source": [
        "plot_results(\"SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRb6CO8OVmPm"
      },
      "source": [
        "##### Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ6tHhvDVmPn"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvaTszlWVmPn"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Expected-SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZVNON6VmPo"
      },
      "source": [
        "##### SARSA de n-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsjYxMauVmPp"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJmVEs9QVmPp"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Sarsa de n-passos (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEUe_Q1k5CA3"
      },
      "source": [
        "### Cliffwalking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24H5nLvV8YQo"
      },
      "outputs": [],
      "source": [
        "# plot_results(\"Linear x Exponencial (Q-Learning) - CliffWalking\",[rewards_linear, rewards_exponential], [\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO-RMfgEC1oJ"
      },
      "source": [
        "##### Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c89bqMd8wVTF"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_qlearning_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_qlearning_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTSe6gLk_YMb"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Q-Learning (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HukOy4ydC5DY"
      },
      "source": [
        "##### Sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLRA4o_dC7nm"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_sarsa_decay(\n",
        "    env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,\n",
        "    initial_E=EPSILON_NORMAL, min_E=EPSILON_START,  # epsilon não decai (fixo)\n",
        "    decay_type=\"none\", decay_rate=0  # sem decaimento\n",
        ")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cvgftBeC8om"
      },
      "outputs": [],
      "source": [
        "plot_results(\"SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UbEk8DaDllr"
      },
      "source": [
        "##### Expected-SARSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mE0CoqzNDqJa"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_expected_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-OmP_4MEP05"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Expected-SARSA (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A862FwMZEerx"
      },
      "source": [
        "##### SARSA de n-passos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TvaKEeMEz4y"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = \"CliffWalking-v0\"\n",
        "EPISODES = 8000\n",
        "LR = 0.01\n",
        "GAMMA = 0.95\n",
        "EPSILON_NORMAL = .005\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_MIN = 0.005\n",
        "DECAY_LINEAR = 0.005\n",
        "DECAY_EXP = 0.005\n",
        "\n",
        "env = gym.make(ENV_NAME, max_episode_steps=500)\n",
        "\n",
        "rewards_linear, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN, decay_type=\"linear\", decay_rate=DECAY_LINEAR)\n",
        "rewards_exponential, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_START, min_E=EPSILON_MIN,decay_type=\"exponential\", decay_rate=DECAY_EXP)\n",
        "\n",
        "# # Executa Q-Learning SEM decaimento (epsilon fixo)\n",
        "rewards_no_decay, _, _, _ = run_nstep_sarsa_decay(env=env, episodes=EPISODES, lr=LR, gamma=GAMMA,initial_E=EPSILON_NORMAL, min_E=EPSILON_START, decay_type=\"none\", decay_rate=0)  # sem decaimento) # epsilon não decai (fixo)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbqsJen5Ekzd"
      },
      "outputs": [],
      "source": [
        "plot_results(\"Sarsa de n-passos (CliffWalking) - Fixo x Linear x Exponencial\",[rewards_no_decay,rewards_linear, rewards_exponential], [\"Fixo\",\"Decaimento Linear\", \"Decaimento Exponencial\"], 0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OqdkF_zjzzdA",
        "MtrRXXsV0JvL",
        "vuh5ELm80k9i",
        "Us3IQDeMx57c",
        "7YBYR3-f9jP3",
        "Ev5B98j6-jxz",
        "Q7-ULM5r0oMa",
        "Ke4_rJ2LDYtF",
        "1sBroZK5IkYj",
        "q40W1P-1L3E7",
        "dFfo3CcmPcTr",
        "UfieBxUT0Qpq",
        "i8CBhgLkbrv0",
        "5hD42QfGbWaZ",
        "wvausVvN3q9F",
        "KRpQH1yC1VOL",
        "aCDsb9CqfmIU",
        "2z1ntbHw8qTg",
        "YeAUDiBSgMAU",
        "QqayEIcpgGld",
        "Gn8K7_bEiVqc",
        "z-lYBrIzVmPf",
        "QEUe_Q1k5CA3",
        "HukOy4ydC5DY",
        "3UbEk8DaDllr",
        "A862FwMZEerx"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}